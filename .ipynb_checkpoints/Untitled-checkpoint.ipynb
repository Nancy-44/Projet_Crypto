{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d56b9afb-5ea7-4112-8301-17a8e41daf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import des librairies et warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Supprime warnings sklearn, xgboost, lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3680b8-c9a1-4933-8185-d65ce7fd6a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   klines_id  symbol_id  interval_id            open_time  \\\n",
      "0          1          3            4  2023-01-30 00:00:00   \n",
      "1          2          3            4  2023-01-31 00:00:00   \n",
      "2          3          3            4  2023-02-01 00:00:00   \n",
      "3          4          3            4  2023-02-02 00:00:00   \n",
      "4          5          3            4  2023-02-03 00:00:00   \n",
      "\n",
      "                close_time   open   high    low  close      volume  \\\n",
      "0  2023-01-30 23:59:59.999  317.1  320.8  302.0  307.2  413716.212   \n",
      "1  2023-01-31 23:59:59.999  307.2  314.4  305.3  312.0  278061.958   \n",
      "2  2023-02-01 23:59:59.999  312.1  319.4  305.6  317.0  390978.523   \n",
      "3  2023-02-02 23:59:59.999  317.1  334.4  316.3  323.5  587655.881   \n",
      "4  2023-02-03 23:59:59.999  323.5  335.5  318.3  332.2  579577.162   \n",
      "\n",
      "                   created_at  \n",
      "0  2025-10-30 14:46:00.254029  \n",
      "1  2025-10-30 14:46:00.254029  \n",
      "2  2025-10-30 14:46:00.254029  \n",
      "3  2025-10-30 14:46:00.254029  \n",
      "4  2025-10-30 14:46:00.254029  \n"
     ]
    }
   ],
   "source": [
    "## Chargement des données\n",
    "df = pd.read_csv(\"dataset_export_1h_4h_1d.csv\")\n",
    "print(df.head())\n",
    "interval_mapping = {2: 60, 3: 240, 4: 1440}  # minutes\n",
    "symbol_mapping = {1: \"BTCUSDT\", 2: \"ETHUSDT\", 3: \"BNBUSDT\", 4: \"SOLUSDT\"}\n",
    "\n",
    "df[\"interval_min\"] = df[\"interval_id\"].map(interval_mapping)\n",
    "df[\"symbol_label\"] = df[\"symbol_id\"].map(symbol_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9545f03a-1801-4d5d-b5d4-adaf0418d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paramètres et création de la target\n",
    "horizon = 10           # horizon de prédiction\n",
    "threshold = 0.01       # seuil de variation pour target binaire\n",
    "\n",
    "df[\"future_return\"] = df[\"close\"].shift(-horizon) / df[\"close\"] - 1\n",
    "df[\"target\"] = np.where(df[\"future_return\"] > threshold, 1,\n",
    "                        np.where(df[\"future_return\"] < -threshold, 0, np.nan))\n",
    "df.dropna(subset=[\"target\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7087995a-220c-496e-857c-6139aa39e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## définition des indicateurs techniques\n",
    "def compute_RSI(close, period=14):\n",
    "    delta = close.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=period, min_periods=period).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_indicators(df):\n",
    "    df = df.copy()\n",
    "    df[\"RSI\"] = compute_RSI(df[\"close\"])\n",
    "    df[\"MA_10\"] = df[\"close\"].rolling(10).mean()\n",
    "    df[\"MA_50\"] = df[\"close\"].rolling(50).mean()\n",
    "    df[\"STD_20\"] = df[\"close\"].rolling(20).std()\n",
    "    df[\"RET\"] = df[\"close\"].pct_change()\n",
    "    df[\"VOLAT\"] = df[\"RET\"].rolling(10).std()\n",
    "    for lag in [1,2,3]:\n",
    "        df[f\"close_lag{lag}\"] = df[\"close\"].shift(lag)\n",
    "        df[f\"volume_lag{lag}\"] = df[\"volume\"].shift(lag)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4031359-e55d-4a6c-96b1-13b9e9e88ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BNBUSDT ===\n",
      "[BASE] LogisticRegression | Acc: 0.543 | F1: 0.651\n",
      "[BASE] RandomForest | Acc: 0.533 | F1: 0.631\n",
      "[BASE] XGBoost      | Acc: 0.560 | F1: 0.684\n",
      "[BASE] LightGBM     | Acc: 0.563 | F1: 0.670\n",
      "[OPT] RandomForest_OPT | Acc: 0.524 | F1: 0.620\n",
      "[OPT] XGBoost_OPT  | Acc: 0.530 | F1: 0.692\n",
      "[OPT] LightGBM_OPT | Acc: 0.532 | F1: 0.692\n",
      "\n",
      "=== ETHUSDT ===\n",
      "[BASE] LogisticRegression | Acc: 0.538 | F1: 0.632\n",
      "[BASE] RandomForest | Acc: 0.560 | F1: 0.635\n",
      "[BASE] XGBoost      | Acc: 0.560 | F1: 0.660\n",
      "[BASE] LightGBM     | Acc: 0.547 | F1: 0.632\n",
      "[OPT] RandomForest_OPT | Acc: 0.558 | F1: 0.632\n",
      "[OPT] XGBoost_OPT  | Acc: 0.525 | F1: 0.684\n",
      "[OPT] LightGBM_OPT | Acc: 0.528 | F1: 0.684\n",
      "\n",
      "=== BTCUSDT ===\n",
      "[BASE] LogisticRegression | Acc: 0.489 | F1: 0.653\n",
      "[BASE] RandomForest | Acc: 0.512 | F1: 0.637\n",
      "[BASE] XGBoost      | Acc: 0.502 | F1: 0.644\n",
      "[BASE] LightGBM     | Acc: 0.493 | F1: 0.631\n",
      "[OPT] RandomForest_OPT | Acc: 0.501 | F1: 0.657\n",
      "[OPT] XGBoost_OPT  | Acc: 0.488 | F1: 0.656\n",
      "[OPT] LightGBM_OPT | Acc: 0.489 | F1: 0.655\n",
      "\n",
      "=== SOLUSDT ===\n",
      "[BASE] LogisticRegression | Acc: 0.534 | F1: 0.657\n",
      "[BASE] RandomForest | Acc: 0.507 | F1: 0.625\n",
      "[BASE] XGBoost      | Acc: 0.506 | F1: 0.628\n",
      "[BASE] LightGBM     | Acc: 0.498 | F1: 0.610\n",
      "[OPT] RandomForest_OPT | Acc: 0.508 | F1: 0.627\n",
      "[OPT] XGBoost_OPT  | Acc: 0.511 | F1: 0.670\n",
      "[OPT] LightGBM_OPT | Acc: 0.510 | F1: 0.663\n"
     ]
    }
   ],
   "source": [
    "## Boucle sur les symboles\n",
    "results = {}\n",
    "symbols = df[\"symbol_label\"].unique()\n",
    "\n",
    "for symbol in symbols:\n",
    "    data = df[df[\"symbol_label\"] == symbol].copy()\n",
    "    if len(data) < 300:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== {symbol} ===\")\n",
    "\n",
    "    # Train/test split avant feature engineering pour éviter leakage\n",
    "    raw_features = data[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].copy()\n",
    "    raw_target = data[\"target\"]\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "        raw_features, raw_target, test_size=0.3, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Calcul des features\n",
    "    train_data = X_train_raw.copy()\n",
    "    train_data[\"target\"] = y_train\n",
    "    train_data = compute_indicators(train_data)\n",
    "    X_train = train_data.drop(columns=[\"target\"])\n",
    "    y_train = train_data[\"target\"]\n",
    "\n",
    "    test_data = X_test_raw.copy()\n",
    "    test_data[\"target\"] = y_test\n",
    "    test_data = compute_indicators(test_data)\n",
    "    X_test = test_data.drop(columns=[\"target\"])\n",
    "    y_test = test_data[\"target\"]\n",
    "\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Gestion déséquilibre\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train)\n",
    "    weight_dict = dict(zip(classes, class_weights))\n",
    "    scale_pos_weight = weight_dict[0] / weight_dict[1]\n",
    "\n",
    "     ## Définition et entraînement des modèles de base\n",
    "    models = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=150, max_depth=6, max_features=\"sqrt\",\n",
    "                                           random_state=42, class_weight=\"balanced\"),\n",
    "        \"XGBoost\": XGBClassifier(n_estimators=150, max_depth=4, learning_rate=0.05,\n",
    "                             subsample=0.8, colsample_bytree=0.8, eval_metric=\"logloss\",\n",
    "                             use_label_encoder=False, random_state=42, scale_pos_weight=scale_pos_weight, verbosity=0),\n",
    "        \"LightGBM\": LGBMClassifier(num_leaves=31, max_depth=5, learning_rate=0.05,\n",
    "                               n_estimators=200, subsample=0.8, colsample_bytree=0.8,\n",
    "                               scale_pos_weight=scale_pos_weight, random_state=42, verbosity=-1)\n",
    "    }\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        scores[name+\"_BASE\"] = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"f1\": f1_score(y_test, y_pred)\n",
    "        }\n",
    "        print(f\"[BASE] {name:12s} | Acc: {scores[name+'_BASE']['accuracy']:.3f} | F1: {scores[name+'_BASE']['f1']:.3f}\")\n",
    "        \n",
    "    ## Optimisation des hyperparamètres\n",
    "    param_rf = {\"n_estimators\":[100,200,300], \"max_depth\":[3,5,6,8], \"min_samples_split\":[2,5], \"max_features\":[\"sqrt\",\"log2\"]}\n",
    "    param_xgb = {\"n_estimators\":[100,200], \"max_depth\":[3,4,5], \"learning_rate\":[0.01,0.05], \"subsample\":[0.7,0.8], \"colsample_bytree\":[0.7,0.8], \"scale_pos_weight\":[scale_pos_weight]}\n",
    "    param_lgb = {\"num_leaves\":[15,31], \"max_depth\":[3,5], \"learning_rate\":[0.01,0.05], \"n_estimators\":[100,200], \"subsample\":[0.7,0.8], \"colsample_bytree\":[0.7,0.8], \"scale_pos_weight\":[scale_pos_weight]}\n",
    "\n",
    "    tuned_models = {\n",
    "        \"RandomForest_OPT\": (RandomForestClassifier(random_state=42, class_weight=\"balanced\"), param_rf),\n",
    "        \"XGBoost_OPT\": (XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False, random_state=42), param_xgb),\n",
    "        \"LightGBM_OPT\": (LGBMClassifier(random_state=42, verbosity=-1), param_lgb)\n",
    "    }\n",
    "\n",
    "    for name, (model, param_grid) in tuned_models.items():\n",
    "        search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=3, scoring=\"f1\", n_jobs=-1, random_state=42)\n",
    "        search.fit(X_train_scaled, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        scores[name] = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"f1\": f1_score(y_test, y_pred)\n",
    "        }\n",
    "        print(f\"[OPT] {name:12s} | Acc: {scores[name]['accuracy']:.3f} | F1: {scores[name]['f1']:.3f}\")\n",
    "        # Sauvegarde modèle optimisé\n",
    "        joblib.dump(best_model, f\"{symbol}_{name}.pkl\")\n",
    "\n",
    "    ## Résultat final\n",
    "    best_model_name = max(scores, key=lambda x: scores[x][\"f1\"])\n",
    "    results[symbol] = {**scores[best_model_name], \"best_model\": best_model_name}\n",
    "print(\"\\n=== Résumé global ===\")\n",
    "for sym, res in results.items():\n",
    "    print(f\"{sym:10s} → {res['best_model']:18s} | Acc {res['accuracy']:.3f} | F1 {res['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95bca63f-bca0-49d7-8fa2-b211567e3363",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (99587861.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mparam_rf = {\"n_estimators\":[100,200,300], \"max_depth\":[3,5,6,8], \"min_samples_split\":[2,5], \"max_features\":[\"sqrt\",\"log2\"]}\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "    ## Optimisation des hyperparamètres\n",
    "    param_rf = {\"n_estimators\":[100,200,300], \"max_depth\":[3,5,6,8], \"min_samples_split\":[2,5], \"max_features\":[\"sqrt\",\"log2\"]}\n",
    "    param_xgb = {\"n_estimators\":[100,200], \"max_depth\":[3,4,5], \"learning_rate\":[0.01,0.05], \"subsample\":[0.7,0.8], \"colsample_bytree\":[0.7,0.8], \"scale_pos_weight\":[scale_pos_weight]}\n",
    "    param_lgb = {\"num_leaves\":[15,31], \"max_depth\":[3,5], \"learning_rate\":[0.01,0.05], \"n_estimators\":[100,200], \"subsample\":[0.7,0.8], \"colsample_bytree\":[0.7,0.8], \"scale_pos_weight\":[scale_pos_weight]}\n",
    "\n",
    "    tuned_models = {\n",
    "        \"RandomForest_OPT\": (RandomForestClassifier(random_state=42, class_weight=\"balanced\"), param_rf),\n",
    "        \"XGBoost_OPT\": (XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False, random_state=42), param_xgb),\n",
    "        \"LightGBM_OPT\": (LGBMClassifier(random_state=42, verbosity=-1), param_lgb)\n",
    "    }\n",
    "\n",
    "    for name, (model, param_grid) in tuned_models.items():\n",
    "        search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=3, scoring=\"f1\", n_jobs=-1, random_state=42)\n",
    "        search.fit(X_train_scaled, y_train)\n",
    "        best_model = search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        scores[name] = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"f1\": f1_score(y_test, y_pred)\n",
    "        }\n",
    "        print(f\"[OPT] {name:12s} | Acc: {scores[name]['accuracy']:.3f} | F1: {scores[name]['f1']:.3f}\")\n",
    "        # Sauvegarde modèle optimisé\n",
    "        joblib.dump(best_model, f\"{symbol}_{name}.pkl\")\n",
    "\n",
    "    ## Résultat final\n",
    "    best_model_name = max(scores, key=lambda x: scores[x][\"f1\"])\n",
    "    results[symbol] = {**scores[best_model_name], \"best_model\": best_model_name}\n",
    "print(\"\\n=== Résumé global ===\")\n",
    "for sym, res in results.items():\n",
    "    print(f\"{sym:10s} → {res['best_model']:18s} | Acc {res['accuracy']:.3f} | F1 {res['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1cacc-be06-4b31-81dc-1bc87cc79272",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Résultat final\n",
    "best_model_name = max(scores, key=lambda x: scores[x][\"f1\"])\n",
    "results[symbol] = {**scores[best_model_name], \"best_model\": best_model_name}\n",
    "\n",
    "print(\"\\n=== Résumé global ===\")\n",
    "for sym, res in results.items():\n",
    "    print(f\"{sym:10s} → {res['best_model']:18s} | Acc {res['accuracy']:.3f} | F1 {res['f1']:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Crypto",
   "language": "python",
   "name": "ml_crypto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
